{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114adf43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando as bibliotecas necessárias\n",
    "import pandas as pd\n",
    "import os\n",
    "import kagglehub\n",
    "import shutil\n",
    "import requests\n",
    "import gzip\n",
    "import duckdb\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bef250",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASETS = [\n",
    "    \"shivamb/amazon-prime-movies-and-tv-shows\",\n",
    "    \"shivamb/disney-movies-and-tv-shows\",\n",
    "    \"shivamb/netflix-shows\",\n",
    "]\n",
    "\n",
    "current_directory = os.getcwd() \n",
    "DESTINATION_DIR = os.path.join(os.path.join(current_directory, '..'), \"data\") \n",
    "\n",
    "# 1. Cria a pasta 'data' se ela não existir\n",
    "os.makedirs(DESTINATION_DIR, exist_ok=True)\n",
    "\n",
    "def download_and_copy_dataset(dataset_name: str, destination_path: str):\n",
    "    \"\"\"Baixa um dataset, copia os arquivos para o destino e limpa o cache.\"\"\"\n",
    "\n",
    "    # 1. Baixa o dataset para o cache\n",
    "    try:\n",
    "        cache_path = kagglehub.dataset_download(dataset_name)\n",
    "    except Exception as e:\n",
    "        return\n",
    "\n",
    "    # 2. Copia os arquivos do cache para o diretório de destino\n",
    "    for item_name in os.listdir(cache_path):\n",
    "        source = os.path.join(cache_path, item_name)\n",
    "        destination = os.path.join(destination_path, item_name)\n",
    "\n",
    "        # Copia apenas arquivos (ignorando subpastas)\n",
    "        if os.path.isfile(source):\n",
    "            shutil.copy2(source, destination) \n",
    "\n",
    "    # 3. Remove completamente a pasta do cache\n",
    "    try:\n",
    "        shutil.rmtree(cache_path)\n",
    "    except OSError as e:\n",
    "        print(f\"  > AVISO: Não foi possível remover o cache: {e}\")\n",
    "        \n",
    "for dataset in DATASETS:\n",
    "    download_and_copy_dataset(dataset, DESTINATION_DIR)\n",
    "\n",
    "print(\"\\n\\n--- Processo Finalizado ---\")\n",
    "print(f\"Todos os arquivos dos datasets estão na pasta: {DESTINATION_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71f7d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def baixar_e_descomprimir_imdb(output_dir=\"../data\"):\n",
    "    \"\"\"\n",
    "    Baixa os arquivos de dataset do IMDb e os descomprime.\n",
    "    Os arquivos são salvos no diretório 'output_dir'.\n",
    "    \"\"\"\n",
    "    \n",
    "    # URL base dos datasets\n",
    "    base_url = \"https://datasets.imdbws.com/\"\n",
    "    \n",
    "    # Arquivos necessários para (Nome da Obra, Nota, Diretor)\n",
    "    files_to_download = [\n",
    "        \"title.basics.tsv.gz\",   # Mapeia tconst -> primaryTitle (Nome da Obra)\n",
    "        \"title.ratings.tsv.gz\",  # Mapeia tconst -> averageRating (Nota)\n",
    "        \"title.crew.tsv.gz\",     # Mapeia tconst -> nconst (ID do Diretor)\n",
    "        \"name.basics.tsv.gz\"     # Mapeia nconst -> primaryName (Nome da Pessoa)\n",
    "    ]\n",
    "    \n",
    "    # Cria o diretório de saída se ele não existir\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "        print(f\"Diretório criado: '{output_dir}'\")\n",
    "\n",
    "    print(\"Iniciando downloads... (Isso pode levar vários minutos por arquivo)\")\n",
    "\n",
    "    for filename in files_to_download:\n",
    "        url = base_url + filename\n",
    "        \n",
    "        gz_path = os.path.join(output_dir, filename)\n",
    "        \n",
    "        tsv_path = gz_path.replace(\".gz\", \"\")\n",
    "        \n",
    "        try:\n",
    "            print(f\"\\nBaixando: {filename}...\")\n",
    "            with requests.get(url, stream=True) as r:\n",
    "                r.raise_for_status() # Lança um erro se o status não for 200\n",
    "                with open(gz_path, 'wb') as f:\n",
    "                    for chunk in r.iter_content(chunk_size=8192): \n",
    "                        f.write(chunk)\n",
    "            print(\"Download concluído.\")\n",
    "\n",
    "            print(f\"Descomprimindo: {filename}...\")\n",
    "            with gzip.open(gz_path, 'rb') as f_in:\n",
    "                with open(tsv_path, 'wb') as f_out:\n",
    "                    shutil.copyfileobj(f_in, f_out)\n",
    "            print(f\"Arquivo salvo: {tsv_path}\")\n",
    "            \n",
    "            os.remove(gz_path)\n",
    "            print(f\"Arquivo temporário '{gz_path}' removido.\")\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"\\nERRO: Falha ao baixar {url}. Motivo: {e}\")\n",
    "            print(\"Por favor, verifique sua conexão ou a URL.\")\n",
    "            if os.path.exists(gz_path):\n",
    "                os.remove(gz_path)\n",
    "        except Exception as e:\n",
    "            print(f\"\\nERRO: Ocorreu um problema: {e}\")\n",
    "\n",
    "    print(f\"\\nProcesso concluído! Os arquivos TSV estão em: '{output_dir}'\")\n",
    "\n",
    "baixar_e_descomprimir_imdb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f215ab55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatenar as base de dados \n",
    "\n",
    "current_directory = os.getcwd() \n",
    "DESTINATION_DIR = os.path.join(os.path.join(current_directory, '..'), \"data\") \n",
    "\n",
    "df_netflix = pd.read_csv(os.path.join(DESTINATION_DIR, 'netflix_titles.csv'))\n",
    "df_disney = pd.read_csv(os.path.join(DESTINATION_DIR, 'disney_plus_titles.csv'))\n",
    "df_amazon = pd.read_csv(os.path.join(DESTINATION_DIR, 'amazon_prime_titles.csv'))\n",
    "\n",
    "df_netflix['streaming'] = 'Netflix'\n",
    "df_disney['streaming'] = 'Disney+'\n",
    "df_amazon['streaming'] = 'Prime Video'\n",
    "\n",
    "dataframes_to_concat = [df_netflix, df_disney, df_amazon]\n",
    "\n",
    "df_streaming = pd.concat(dataframes_to_concat, ignore_index=True)\n",
    "\n",
    "df_streaming['date_added'] = df_streaming['date_added'].str.strip()\n",
    "\n",
    "df_streaming['date_added'] = pd.to_datetime(df_streaming['date_added'], format='%B %d, %Y')\n",
    "\n",
    "# Exibindo informações gerais para confirmar a junção e os tipos de dados\n",
    "print(\"\\nInformações do DataFrame consolidado:\")\n",
    "df_streaming.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc2033c",
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_mapping = {\n",
    "    # --- PADRONIZAÇÃO E CONSOLIDAÇÃO (TV/Filmes -> Gênero Principal) ---\n",
    "    'Dramas': 'Drama', 'TV Dramas': 'Drama',\n",
    "    'Comedies': 'Comedy', 'TV Comedies': 'Comedy', 'Romantic Comedy': 'Romance , Comedy',\n",
    "    'Thrillers': 'Thriller', 'TV Thrillers': 'Thriller',\n",
    "    'Documentaries': 'Documentary', 'Docuseries': 'Documentary',\n",
    "    'Horror Movies': 'Horror', 'TV Horror': 'Horror',\n",
    "    'Romantic Movies': 'Romance', 'Romantic TV Shows': 'Romance',\n",
    "    'International Movies': 'International', 'International TV Shows': 'International',\n",
    "    'Independent Movies': 'Independent',\n",
    "    'Sports Movies': 'Sports',\n",
    "    'Classic Movies': 'Classic',\n",
    "    'Cult Movies': 'Cult',\n",
    "    'LGBTQ Movies': 'LGBTQ',\n",
    "    'Crime TV Shows': 'Crime',\n",
    "    'Reality TV': 'Reality',\n",
    "    'Teen TV Shows': 'Teen',\n",
    "    'TV Mysteries': 'Mystery',\n",
    "    'Science Fiction': 'Sci-Fi',\n",
    "\n",
    "    # --- MAPEAMENTO DE SINÔNIMOS E SUB-GÊNEROS ---\n",
    "    'Anime Features': 'Anime', 'Anime Series': 'Anime',\n",
    "    \"Kids' TV\": \"Kids\", 'Children & Family Movies': 'Kids, Family',\n",
    "    'Faith and Spirituality': 'Faith & Spirituality',\n",
    "    'Young Adult Audience': 'Young Adult',\n",
    "    'Soap Opera / Melodrama': 'Soap Opera',\n",
    "    'and Culture': 'Culture', \n",
    "\n",
    "    # --- SEPARAÇÃO DE GÊNEROS COMPOSTOS (usando vírgula) ---\n",
    "    'Animals & Nature': 'Nature',\n",
    "    'Science & Nature': 'Nature, Science', \n",
    "    'Arts & Culture': 'Culture, Art',\n",
    "    'Action & Adventure': 'Action, Adventure',\n",
    "    'Sci-Fi & Fantasy': 'Sci-Fi, Fantasy',\n",
    "    'Stand-Up Comedy & Talk Shows': 'Stand-Up Comedy, Talk Show',\n",
    "    'Music Videos and Concerts': 'Music',\n",
    "    'Music & Musicals': 'Music, Musical',\n",
    "    'Science & Nature TV': 'Science, Nature',\n",
    "    'Animals & Nature': 'Animals, Nature',\n",
    "    'TV Action & Adventure': 'Action, Adventure',\n",
    "    'TV Sci-Fi & Fantasy': 'Sci-Fi, Fantasy',\n",
    "    'Game Show / Competition': 'Game Show, Competition',\n",
    "    'Action-Adventure': 'Action, Adventure',\n",
    "    'Classic & Cult TV': 'Classic, Cult',\n",
    "    'Talk Show and Variety': 'Talk Show, Variety',\n",
    "    \n",
    "    # --- Mapeamento direto de gêneros de TV para manter a distinção se desejado ---\n",
    "    'Korean TV Shows': 'Korean TV',\n",
    "    'British TV Shows': 'British TV',\n",
    "    'Spanish-Language TV Shows': 'Spanish TV',\n",
    "\n",
    "    # --- Remoção de Formatos (não são gêneros temáticos) ---\n",
    "    'Movies': '_REMOVE_',\n",
    "    'Series': '_REMOVE_',\n",
    "    'TV Shows': '_REMOVE_', \n",
    "    'TV Show': '_REMOVE_',\n",
    "    'Anthology': '_REMOVE_',\n",
    "    'Unscripted': '_REMOVE_', # Categoria muito ampla, coberta por Reality\n",
    "    'Special Interest': '_REMOVE_' # Categoria muito genérica\n",
    "}\n",
    "\n",
    "def process_genres(genre_string):\n",
    "    \"\"\"\n",
    "    Função para aplicar o mapeamento de gênero em uma string \n",
    "    que pode conter múltiplos gêneros.\n",
    "    \"\"\"\n",
    "    if pd.isna(genre_string):\n",
    "        return '' # Retorna string vazia \n",
    "\n",
    "    processed_genres = set()\n",
    "    \n",
    "    # 1. Separa os gêneros da string original (ex: \"TV Dramas, TV Mysteries\")\n",
    "    initial_genres = genre_string.split(',')\n",
    "\n",
    "    for genre in initial_genres:\n",
    "        # 2. Limpa o whitespace (ex: \" TV Dramas\" -> \"TV Dramas\")\n",
    "        clean_genre = genre.strip()\n",
    "\n",
    "        # 3. Aplica o mapping. \n",
    "        mapped_value = genre_mapping.get(clean_genre, clean_genre)\n",
    "\n",
    "        # 4. Processa o valor mapeado\n",
    "        if mapped_value == '_REMOVE_':\n",
    "            # Não faz nada, simplesmente ignora o gênero\n",
    "            continue\n",
    "        elif ',' in mapped_value:\n",
    "            # Separa, limpa e adiciona cada sub-gênero\n",
    "            sub_genres = mapped_value.split(',')\n",
    "            for sub in sub_genres:\n",
    "                processed_genres.add(sub.strip())\n",
    "        else:\n",
    "            # É um valor único, não vazio e não _REMOVE_\n",
    "            if mapped_value:\n",
    "                processed_genres.add(mapped_value)\n",
    "    \n",
    "    # ordenados alfabeticamente para consistência.\n",
    "    return ', '.join(sorted(list(processed_genres)))\n",
    "\n",
    "df_streaming['genres_processed'] = df_streaming['listed_in'].apply(process_genres)\n",
    "\n",
    "# 4. (Opcional) Mostra o resultado das 10 primeiras linhas\n",
    "print(\"Processamento concluído. Exemplo do resultado:\")\n",
    "print(df_streaming[['listed_in', 'genres_processed']].head(10))\n",
    "\n",
    "print(df_streaming.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c938e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_csv_file = os.path.join('../data', 'filmes_series.csv')\n",
    "df_streaming.to_csv(output_csv_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a73cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_data_dir = '../data'\n",
    "basics_tsv = os.path.join(imdb_data_dir, 'title.basics.tsv')\n",
    "ratings_tsv = os.path.join(imdb_data_dir, 'title.ratings.tsv')\n",
    "crew_tsv = os.path.join(imdb_data_dir, 'title.crew.tsv')\n",
    "names_tsv = os.path.join(imdb_data_dir, 'name.basics.tsv')\n",
    "movies_csv_file = os.path.join(imdb_data_dir, 'filmes_series.csv')\n",
    "output_csv_file = os.path.join(imdb_data_dir, 'filmes_series_imdb.csv')\n",
    "\n",
    "con = duckdb.connect(database=':memory:', read_only=False)\n",
    "\n",
    "# --- Consulta SQL Principal ---\n",
    "# Esta consulta é longa, pois prepara e junta 5 arquivos diferentes.\n",
    "# Consulta desenvolvida com auxilio de IA\n",
    "merge_query = f\"\"\"\n",
    "WITH\n",
    "-- 1. Prepara a base de Títulos e Notas do IMDb\n",
    "imdb_titles_with_rating AS (\n",
    "    SELECT\n",
    "        basics.tconst,\n",
    "        basics.primaryTitle,\n",
    "        basics.titleType,\n",
    "        basics.runtimeMinutes,\n",
    "        ratings.averageRating,\n",
    "        ratings.numVotes,\n",
    "        TRY_CAST(basics.startYear AS INT64) AS startYear\n",
    "    FROM read_csv_auto('{basics_tsv}', header=True, delim='\\t', quote='', strict_mode=False) AS basics\n",
    "    JOIN read_csv_auto('{ratings_tsv}', header=True, delim='\\t', quote='') AS ratings\n",
    "        ON basics.tconst = ratings.tconst\n",
    "    WHERE basics.titleType IN ('movie', 'tvSeries', 'tvMiniSeries', 'tvMovie')\n",
    "),\n",
    "\n",
    "-- 2. Prepara a base de Diretores do IMDb \n",
    "imdb_director_names AS (\n",
    "    SELECT\n",
    "        crew.tconst,\n",
    "        names.primaryName AS imdb_director_name\n",
    "    FROM read_csv_auto('{crew_tsv}', header=True, delim='\\t', quote='') AS crew\n",
    "    CROSS JOIN unnest(string_split(crew.directors, ',')) AS t(nconst_id)\n",
    "    JOIN read_csv_auto('{names_tsv}', header=True, delim='\\t', quote='', strict_mode=False) AS names\n",
    "        ON names.nconst = t.nconst_id\n",
    "    WHERE t.nconst_id != '\\\\N'\n",
    "),\n",
    "\n",
    "-- 3. Junta Títulos, Notas e Nomes de Diretores do IMDb \n",
    "imdb_full_directors AS (\n",
    "    SELECT\n",
    "        LOWER(TRIM(t.primaryTitle)) AS imdb_title_clean,\n",
    "        LOWER(TRIM(d.imdb_director_name)) AS imdb_director_clean,\n",
    "        t.averageRating AS imdb_rating,\n",
    "        t.runtimeMinutes AS run_time_minutes,\n",
    "        t.numVotes AS number_votes\n",
    "    FROM imdb_titles_with_rating AS t\n",
    "    JOIN imdb_director_names AS d ON t.tconst = d.tconst\n",
    "    GROUP BY 1, 2, 3, 4, 5\n",
    "),\n",
    "\n",
    "-- 4. Prepara a base de dados (sem alteração, com UNION ALL)\n",
    "netflix_directors_exploded AS (\n",
    "    SELECT\n",
    "        n.*, \n",
    "        LOWER(TRIM(director_name)) AS director_clean,\n",
    "        LOWER(TRIM(title)) AS title_clean\n",
    "    FROM read_csv_auto('{movies_csv_file}', header=True, auto_detect=True) AS n\n",
    "    CROSS JOIN unnest(string_split(n.director, ',')) AS t(director_name)\n",
    "    WHERE n.director IS NOT NULL\n",
    "    UNION ALL\n",
    "    SELECT\n",
    "        n.*, \n",
    "        NULL AS director_clean, \n",
    "        LOWER(TRIM(title)) AS title_clean\n",
    "    FROM read_csv_auto('{movies_csv_file}', header=True, auto_detect=True) AS n\n",
    "    WHERE n.director IS NULL\n",
    "),\n",
    "\n",
    "-- 5. Prepara a base do IMDb para junção por Título + Ano\n",
    "imdb_full_year AS (\n",
    "    SELECT\n",
    "        LOWER(TRIM(primaryTitle)) AS imdb_title_clean,\n",
    "        startYear,\n",
    "        AVG(averageRating) AS imdb_rating -- Média caso haja duplicatas (raro)\n",
    "    FROM imdb_titles_with_rating\n",
    "    GROUP BY 1, 2\n",
    ")\n",
    "\n",
    "\n",
    "-- 6. Consulta Final: Junta dataset com IMDb \n",
    "SELECT\n",
    "    n.show_id,\n",
    "    n.type,\n",
    "    n.title,\n",
    "    n.director,\n",
    "    n.cast,\n",
    "    n.country,\n",
    "    n.date_added,\n",
    "    n.release_year,\n",
    "    n.rating,\n",
    "    n.duration,\n",
    "    n.listed_in,\n",
    "    n.description,\n",
    "    n.genres_processed,\n",
    "    n.streaming,\n",
    "    \n",
    "    -- COALESCE usa a primeira nota que não for NULA\n",
    "    COALESCE(\n",
    "        AVG(i_director.imdb_rating), -- 1ª Tentativa: Título + Diretor\n",
    "        AVG(i_year.imdb_rating)      -- 2ª Tentativa: Título + Ano\n",
    "    ) AS imdb_rating_concatenada,\n",
    "\n",
    "    i_director.run_time_minutes,\n",
    "    i_director.number_votes\n",
    "\n",
    "    \n",
    "FROM netflix_directors_exploded AS n\n",
    "-- JOIN 1: Título + Diretor (Alta Precisão)\n",
    "LEFT JOIN imdb_full_directors AS i_director\n",
    "    ON n.title_clean = i_director.imdb_title_clean\n",
    "    AND n.director_clean = i_director.imdb_director_clean\n",
    "\n",
    "-- JOIN 2: Título + Ano (Média Precisão)\n",
    "LEFT JOIN imdb_full_year AS i_year\n",
    "    ON n.title_clean = i_year.imdb_title_clean\n",
    "    AND jaro_winkler_similarity(n.title_clean, i_year.imdb_title_clean) > 0.8\n",
    "\n",
    "GROUP BY\n",
    "    n.show_id, n.type, n.title, n.director, n.cast, n.country,\n",
    "    n.date_added, n.release_year, n.rating, n.duration,\n",
    "    n.listed_in, n.description, n.streaming, n.genres_processed, \n",
    "    i_director.run_time_minutes, i_director.number_votes\n",
    "\n",
    "\n",
    "HAVING\n",
    "    COALESCE(AVG(i_director.imdb_rating), AVG(i_year.imdb_rating)) IS NOT NULL\n",
    "\"\"\"\n",
    "\n",
    "print(\"Executando a consulta de 'merge' (isso pode levar alguns minutos)...\")\n",
    "# Executa a consulta e salva o resultado em um novo CSV\n",
    "con.execute(f\"\"\"\n",
    "    COPY ({merge_query})\n",
    "    TO '{output_csv_file}'\n",
    "    WITH (HEADER 1, DELIMITER ',')\n",
    "\"\"\")\n",
    "print(f\"\\nSucesso! Arquivo salvo como: '{output_csv_file}'\")\n",
    "\n",
    "# --- Verificação ---\n",
    "print(\"Verificando quantos títulos conseguimos concatenar...\")\n",
    "\n",
    "result = con.execute(f\"\"\"\n",
    "    SELECT\n",
    "        COUNT(*) AS total_titulos,\n",
    "        COUNT(imdb_rating_concatenada) AS titulos_com_nota\n",
    "    FROM read_csv_auto('{output_csv_file}', header=True)\n",
    "\"\"\").df()\n",
    "\n",
    "total = df_streaming.shape[0]\n",
    "matched = result['titulos_com_nota'].iloc[0]\n",
    "percent = (matched / total) * 100 if total > 0 else 0\n",
    "\n",
    "print(\"\\n--- Relatório de Concatenação ---\")\n",
    "print(f\"Total de títulos no seu CSV: {total}\")\n",
    "print(f\"Títulos que receberam nota do IMDb: {matched}\")\n",
    "print(f\"Taxa de sucesso da concatenação: {percent:.2f}%\")\n",
    "\n",
    "\n",
    "#df = pd.read_csv(output_csv_file)\n",
    "#df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933f0909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Carregar os dados\n",
    "imdb_data_dir = '../data/'\n",
    "output_csv_file = os.path.join(imdb_data_dir, 'filmes_series_imdb.csv')\n",
    "df = pd.read_csv(output_csv_file)\n",
    "\n",
    "# Função auxiliar para limpeza (DRY - Don't Repeat Yourself)\n",
    "def limpar_coluna_numerica(series):\n",
    "    # Converte para string, remove '\\n' e espaços extras\n",
    "    s = series.astype(str).str.replace(r'\\n', '', regex=True).str.strip()\n",
    "    # Converte para número, transformando erros (como 'nan' string) em NaN real\n",
    "    return pd.to_numeric(s, errors='coerce')\n",
    "\n",
    "# 2. Aplica a limpeza\n",
    "print(\"Limpando colunas...\")\n",
    "df['run_time_minutes'] = limpar_coluna_numerica(df['run_time_minutes'])\n",
    "df['number_votes'] = limpar_coluna_numerica(df['number_votes'])\n",
    "\n",
    "# 3. Tratamento de NaN (Valores Nulos)\n",
    "\n",
    "# Usamos mediana porque a média é sensível a outliers (filmes de 10h ou 1min)\n",
    "mediana_tempo = df['run_time_minutes'].median()\n",
    "df['run_time_minutes'] = df['run_time_minutes'].fillna(mediana_tempo)\n",
    "print(f\"Valores nulos em 'run_time_minutes' preenchidos com a mediana: {mediana_tempo}\")\n",
    "\n",
    "# Se não tem votos registrados, assumir 0 é uma escolha segura e lógica.\n",
    "df['number_votes'] = df['number_votes'].fillna(0)\n",
    "print(\"Valores nulos em 'number_votes' preenchidos com 0.\")\n",
    "\n",
    "# 4. Verificação Final\n",
    "print(\"\\nInfo após tratamento:\")\n",
    "print(df[['run_time_minutes', 'number_votes']].info())\n",
    "print(df[['run_time_minutes', 'number_votes']].head())\n",
    "\n",
    "# 5. Salvar\n",
    "df.to_csv(output_csv_file, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
