{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114adf43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando as bibliotecas necessárias\n",
    "import pandas as pd\n",
    "import os\n",
    "import kagglehub\n",
    "import shutil\n",
    "import requests\n",
    "import gzip\n",
    "import duckdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bef250",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASETS = [\n",
    "    \"shivamb/amazon-prime-movies-and-tv-shows\",\n",
    "    \"shivamb/disney-movies-and-tv-shows\",\n",
    "    \"shivamb/netflix-shows\",\n",
    "]\n",
    "\n",
    "current_directory = os.getcwd() \n",
    "DESTINATION_DIR = os.path.join(current_directory, \"data\") \n",
    "\n",
    "# 1. Cria a pasta 'data' se ela não existir\n",
    "os.makedirs(DESTINATION_DIR, exist_ok=True)\n",
    "\n",
    "def download_and_copy_dataset(dataset_name: str, destination_path: str):\n",
    "    \"\"\"Baixa um dataset, copia os arquivos para o destino e limpa o cache.\"\"\"\n",
    "\n",
    "    # 1. Baixa o dataset para o cache\n",
    "    try:\n",
    "        cache_path = kagglehub.dataset_download(dataset_name)\n",
    "    except Exception as e:\n",
    "        return\n",
    "\n",
    "    # 2. Copia os arquivos do cache para o diretório de destino\n",
    "    for item_name in os.listdir(cache_path):\n",
    "        source = os.path.join(cache_path, item_name)\n",
    "        destination = os.path.join(destination_path, item_name)\n",
    "\n",
    "        # Copia apenas arquivos (ignorando subpastas)\n",
    "        if os.path.isfile(source):\n",
    "            shutil.copy2(source, destination) \n",
    "\n",
    "    # 3. Remove completamente a pasta do cache\n",
    "    try:\n",
    "        shutil.rmtree(cache_path)\n",
    "    except OSError as e:\n",
    "        print(f\"  > AVISO: Não foi possível remover o cache: {e}\")\n",
    "        \n",
    "for dataset in DATASETS:\n",
    "    download_and_copy_dataset(dataset, DESTINATION_DIR)\n",
    "\n",
    "print(\"\\n\\n--- Processo Finalizado ---\")\n",
    "print(f\"Todos os arquivos dos datasets estão na pasta: {DESTINATION_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71f7d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def baixar_e_descomprimir_imdb(output_dir=\"data\"):\n",
    "    \"\"\"\n",
    "    Baixa os arquivos de dataset do IMDb e os descomprime.\n",
    "    Os arquivos são salvos no diretório 'output_dir'.\n",
    "    \"\"\"\n",
    "    \n",
    "    # URL base dos datasets\n",
    "    base_url = \"https://datasets.imdbws.com/\"\n",
    "    \n",
    "    # Arquivos necessários para (Nome da Obra, Nota, Diretor)\n",
    "    files_to_download = [\n",
    "        \"title.basics.tsv.gz\",   # Mapeia tconst -> primaryTitle (Nome da Obra)\n",
    "        \"title.ratings.tsv.gz\",  # Mapeia tconst -> averageRating (Nota)\n",
    "        \"title.crew.tsv.gz\",     # Mapeia tconst -> nconst (ID do Diretor)\n",
    "        \"name.basics.tsv.gz\"     # Mapeia nconst -> primaryName (Nome da Pessoa)\n",
    "    ]\n",
    "    \n",
    "    # Cria o diretório de saída se ele não existir\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "        print(f\"Diretório criado: '{output_dir}'\")\n",
    "\n",
    "    print(\"Iniciando downloads... (Isso pode levar vários minutos por arquivo)\")\n",
    "\n",
    "    for filename in files_to_download:\n",
    "        url = base_url + filename\n",
    "        \n",
    "        gz_path = os.path.join(output_dir, filename)\n",
    "        \n",
    "        tsv_path = gz_path.replace(\".gz\", \"\")\n",
    "        \n",
    "        try:\n",
    "            print(f\"\\nBaixando: {filename}...\")\n",
    "            with requests.get(url, stream=True) as r:\n",
    "                r.raise_for_status() # Lança um erro se o status não for 200\n",
    "                with open(gz_path, 'wb') as f:\n",
    "                    for chunk in r.iter_content(chunk_size=8192): \n",
    "                        f.write(chunk)\n",
    "            print(\"Download concluído.\")\n",
    "\n",
    "            print(f\"Descomprimindo: {filename}...\")\n",
    "            with gzip.open(gz_path, 'rb') as f_in:\n",
    "                with open(tsv_path, 'wb') as f_out:\n",
    "                    shutil.copyfileobj(f_in, f_out)\n",
    "            print(f\"Arquivo salvo: {tsv_path}\")\n",
    "            \n",
    "            os.remove(gz_path)\n",
    "            print(f\"Arquivo temporário '{gz_path}' removido.\")\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"\\nERRO: Falha ao baixar {url}. Motivo: {e}\")\n",
    "            print(\"Por favor, verifique sua conexão ou a URL.\")\n",
    "            if os.path.exists(gz_path):\n",
    "                os.remove(gz_path)\n",
    "        except Exception as e:\n",
    "            print(f\"\\nERRO: Ocorreu um problema: {e}\")\n",
    "\n",
    "    print(f\"\\nProcesso concluído! Os arquivos TSV estão em: '{output_dir}'\")\n",
    "\n",
    "baixar_e_descomprimir_imdb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f215ab55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatenar as base de dados \n",
    "\n",
    "current_directory = os.getcwd() \n",
    "DESTINATION_DIR = os.path.join(current_directory, \"data\") \n",
    "\n",
    "df_netflix = pd.read_csv(os.path.join(DESTINATION_DIR, 'netflix_titles.csv'))\n",
    "df_disney = pd.read_csv(os.path.join(DESTINATION_DIR, 'disney_plus_titles.csv'))\n",
    "df_amazon = pd.read_csv(os.path.join(DESTINATION_DIR, 'amazon_prime_titles.csv'))\n",
    "\n",
    "df_netflix['streaming'] = 'Netflix'\n",
    "df_disney['streaming'] = 'Disney+'\n",
    "df_amazon['streaming'] = 'Prime Video'\n",
    "\n",
    "dataframes_to_concat = [df_netflix, df_disney, df_amazon]\n",
    "\n",
    "df_streaming = pd.concat(dataframes_to_concat, ignore_index=True)\n",
    "\n",
    "df_streaming['date_added'] = df_streaming['date_added'].str.strip()\n",
    "\n",
    "df_streaming['date_added'] = pd.to_datetime(df_streaming['date_added'], format='%B %d, %Y')\n",
    "\n",
    "# Exibindo informações gerais para confirmar a junção e os tipos de dados\n",
    "print(\"\\nInformações do DataFrame consolidado:\")\n",
    "df_streaming.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc2033c",
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_mapping = {\n",
    "    # --- PADRONIZAÇÃO E CONSOLIDAÇÃO (TV/Filmes -> Gênero Principal) ---\n",
    "    'Dramas': 'Drama', 'TV Dramas': 'Drama',\n",
    "    'Comedies': 'Comedy', 'TV Comedies': 'Comedy', 'Romantic Comedy': 'Romance , Comedy',\n",
    "    'Thrillers': 'Thriller', 'TV Thrillers': 'Thriller',\n",
    "    'Documentaries': 'Documentary', 'Docuseries': 'Documentary',\n",
    "    'Horror Movies': 'Horror', 'TV Horror': 'Horror',\n",
    "    'Romantic Movies': 'Romance', 'Romantic TV Shows': 'Romance',\n",
    "    'International Movies': 'International', 'International TV Shows': 'International',\n",
    "    'Independent Movies': 'Independent',\n",
    "    'Sports Movies': 'Sports',\n",
    "    'Classic Movies': 'Classic',\n",
    "    'Cult Movies': 'Cult',\n",
    "    'LGBTQ Movies': 'LGBTQ',\n",
    "    'Crime TV Shows': 'Crime',\n",
    "    'Reality TV': 'Reality',\n",
    "    'Teen TV Shows': 'Teen',\n",
    "    'TV Mysteries': 'Mystery',\n",
    "    'Science Fiction': 'Sci-Fi',\n",
    "\n",
    "    # --- MAPEAMENTO DE SINÔNIMOS E SUB-GÊNEROS ---\n",
    "    'Anime Features': 'Anime', 'Anime Series': 'Anime',\n",
    "    \"Kids' TV\": \"Kids\", 'Children & Family Movies': 'Kids, Family',\n",
    "    'Faith and Spirituality': 'Faith & Spirituality',\n",
    "    'Young Adult Audience': 'Young Adult',\n",
    "    'Soap Opera / Melodrama': 'Soap Opera',\n",
    "    'and Culture': 'Culture', \n",
    "\n",
    "    # --- SEPARAÇÃO DE GÊNEROS COMPOSTOS (usando vírgula) ---\n",
    "    'Animals & Nature': 'Nature',\n",
    "    'Science & Nature': 'Nature, Science', \n",
    "    'Arts & Culture': 'Culture, Art',\n",
    "    'Action & Adventure': 'Action, Adventure',\n",
    "    'Sci-Fi & Fantasy': 'Sci-Fi, Fantasy',\n",
    "    'Stand-Up Comedy & Talk Shows': 'Stand-Up Comedy, Talk Show',\n",
    "    'Music Videos and Concerts': 'Music',\n",
    "    'Music & Musicals': 'Music, Musical',\n",
    "    'Science & Nature TV': 'Science, Nature',\n",
    "    'Animals & Nature': 'Animals, Nature',\n",
    "    'TV Action & Adventure': 'Action, Adventure',\n",
    "    'TV Sci-Fi & Fantasy': 'Sci-Fi, Fantasy',\n",
    "    'Game Show / Competition': 'Game Show, Competition',\n",
    "    'Action-Adventure': 'Action, Adventure',\n",
    "    'Classic & Cult TV': 'Classic, Cult',\n",
    "    'Talk Show and Variety': 'Talk Show, Variety',\n",
    "    \n",
    "    # --- Mapeamento direto de gêneros de TV para manter a distinção se desejado ---\n",
    "    'Korean TV Shows': 'Korean TV',\n",
    "    'British TV Shows': 'British TV',\n",
    "    'Spanish-Language TV Shows': 'Spanish TV',\n",
    "\n",
    "    # --- Remoção de Formatos (não são gêneros temáticos) ---\n",
    "    'Movies': '_REMOVE_',\n",
    "    'Series': '_REMOVE_',\n",
    "    'TV Shows': '_REMOVE_', \n",
    "    'TV Show': '_REMOVE_',\n",
    "    'Anthology': '_REMOVE_',\n",
    "    'Unscripted': '_REMOVE_', # Categoria muito ampla, coberta por Reality\n",
    "    'Special Interest': '_REMOVE_' # Categoria muito genérica\n",
    "}\n",
    "\n",
    "def process_genres(genre_string):\n",
    "    \"\"\"\n",
    "    Função para aplicar o mapeamento de gênero em uma string \n",
    "    que pode conter múltiplos gêneros.\n",
    "    \"\"\"\n",
    "    if pd.isna(genre_string):\n",
    "        return '' # Retorna string vazia \n",
    "\n",
    "    processed_genres = set()\n",
    "    \n",
    "    # 1. Separa os gêneros da string original (ex: \"TV Dramas, TV Mysteries\")\n",
    "    initial_genres = genre_string.split(',')\n",
    "\n",
    "    for genre in initial_genres:\n",
    "        # 2. Limpa o whitespace (ex: \" TV Dramas\" -> \"TV Dramas\")\n",
    "        clean_genre = genre.strip()\n",
    "\n",
    "        # 3. Aplica o mapping. \n",
    "        mapped_value = genre_mapping.get(clean_genre, clean_genre)\n",
    "\n",
    "        # 4. Processa o valor mapeado\n",
    "        if mapped_value == '_REMOVE_':\n",
    "            # Não faz nada, simplesmente ignora o gênero\n",
    "            continue\n",
    "        elif ',' in mapped_value:\n",
    "            # Separa, limpa e adiciona cada sub-gênero\n",
    "            sub_genres = mapped_value.split(',')\n",
    "            for sub in sub_genres:\n",
    "                processed_genres.add(sub.strip())\n",
    "        else:\n",
    "            # É um valor único, não vazio e não _REMOVE_\n",
    "            if mapped_value:\n",
    "                processed_genres.add(mapped_value)\n",
    "    \n",
    "    # ordenados alfabeticamente para consistência.\n",
    "    return ', '.join(sorted(list(processed_genres)))\n",
    "\n",
    "df_streaming['genres_processed'] = df_streaming['listed_in'].apply(process_genres)\n",
    "\n",
    "# 4. (Opcional) Mostra o resultado das 10 primeiras linhas\n",
    "print(\"Processamento concluído. Exemplo do resultado:\")\n",
    "print(df_streaming[['listed_in', 'genres_processed']].head(10))\n",
    "\n",
    "print(df_streaming.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c938e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_csv_file = os.path.join('data', 'filmes_series.csv')\n",
    "df_streaming.to_csv(output_csv_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a73cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_data_dir = 'data'\n",
    "basics_tsv = os.path.join(imdb_data_dir, 'title.basics.tsv')\n",
    "ratings_tsv = os.path.join(imdb_data_dir, 'title.ratings.tsv')\n",
    "crew_tsv = os.path.join(imdb_data_dir, 'title.crew.tsv')\n",
    "names_tsv = os.path.join(imdb_data_dir, 'name.basics.tsv')\n",
    "movies_csv_file = os.path.join(imdb_data_dir, 'filmes_series.csv')\n",
    "output_csv_file = os.path.join(imdb_data_dir, 'filmes_series_imdb.csv')\n",
    "\n",
    "con = duckdb.connect(database=':memory:', read_only=False)\n",
    "\n",
    "# --- Consulta SQL Principal ---\n",
    "# Esta consulta é longa, pois prepara e junta 5 arquivos diferentes.\n",
    "# Consulta desenvolvida com auxilio de IA\n",
    "merge_query = f\"\"\"\n",
    "WITH\n",
    "-- 1. Prepara a base de Títulos e Notas do IMDb\n",
    "imdb_titles_with_rating AS (\n",
    "    SELECT\n",
    "        basics.tconst,\n",
    "        basics.primaryTitle,\n",
    "        basics.titleType,\n",
    "        ratings.averageRating,\n",
    "        TRY_CAST(basics.startYear AS INT64) AS startYear\n",
    "    FROM read_csv_auto('{basics_tsv}', header=True, delim='\\t', quote='', strict_mode=False) AS basics\n",
    "    JOIN read_csv_auto('{ratings_tsv}', header=True, delim='\\t', quote='') AS ratings\n",
    "        ON basics.tconst = ratings.tconst\n",
    "    WHERE basics.titleType IN ('movie', 'tvSeries', 'tvMiniSeries', 'tvMovie')\n",
    "),\n",
    "\n",
    "-- 2. Prepara a base de Diretores do IMDb \n",
    "imdb_director_names AS (\n",
    "    SELECT\n",
    "        crew.tconst,\n",
    "        names.primaryName AS imdb_director_name\n",
    "    FROM read_csv_auto('{crew_tsv}', header=True, delim='\\t', quote='') AS crew\n",
    "    CROSS JOIN unnest(string_split(crew.directors, ',')) AS t(nconst_id)\n",
    "    JOIN read_csv_auto('{names_tsv}', header=True, delim='\\t', quote='', strict_mode=False) AS names\n",
    "        ON names.nconst = t.nconst_id\n",
    "    WHERE t.nconst_id != '\\\\N'\n",
    "),\n",
    "\n",
    "-- 3. Junta Títulos, Notas e Nomes de Diretores do IMDb \n",
    "imdb_full_directors AS (\n",
    "    SELECT\n",
    "        LOWER(TRIM(t.primaryTitle)) AS imdb_title_clean,\n",
    "        LOWER(TRIM(d.imdb_director_name)) AS imdb_director_clean,\n",
    "        t.averageRating AS imdb_rating\n",
    "    FROM imdb_titles_with_rating AS t\n",
    "    JOIN imdb_director_names AS d ON t.tconst = d.tconst\n",
    "    GROUP BY 1, 2, 3\n",
    "),\n",
    "\n",
    "-- 4. Prepara a base de dados (sem alteração, com UNION ALL)\n",
    "netflix_directors_exploded AS (\n",
    "    SELECT\n",
    "        n.*, \n",
    "        LOWER(TRIM(director_name)) AS director_clean,\n",
    "        LOWER(TRIM(title)) AS title_clean\n",
    "    FROM read_csv_auto('{movies_csv_file}', header=True, auto_detect=True) AS n\n",
    "    CROSS JOIN unnest(string_split(n.director, ',')) AS t(director_name)\n",
    "    WHERE n.director IS NOT NULL\n",
    "    UNION ALL\n",
    "    SELECT\n",
    "        n.*, \n",
    "        NULL AS director_clean, \n",
    "        LOWER(TRIM(title)) AS title_clean\n",
    "    FROM read_csv_auto('{movies_csv_file}', header=True, auto_detect=True) AS n\n",
    "    WHERE n.director IS NULL\n",
    "),\n",
    "\n",
    "-- 5. Prepara a base do IMDb para junção por Título + Ano\n",
    "imdb_full_year AS (\n",
    "    SELECT\n",
    "        LOWER(TRIM(primaryTitle)) AS imdb_title_clean,\n",
    "        startYear,\n",
    "        AVG(averageRating) AS imdb_rating -- Média caso haja duplicatas (raro)\n",
    "    FROM imdb_titles_with_rating\n",
    "    GROUP BY 1, 2\n",
    ")\n",
    "\n",
    "\n",
    "-- 6. Consulta Final: Junta dataset com IMDb \n",
    "SELECT\n",
    "    n.show_id,\n",
    "    n.type,\n",
    "    n.title,\n",
    "    n.director,\n",
    "    n.cast,\n",
    "    n.country,\n",
    "    n.date_added,\n",
    "    n.release_year,\n",
    "    n.rating,\n",
    "    n.duration,\n",
    "    n.listed_in,\n",
    "    n.description,\n",
    "    n.genres_processed,\n",
    "    n.streaming,\n",
    "    \n",
    "    -- COALESCE usa a primeira nota que não for NULA\n",
    "    COALESCE(\n",
    "        AVG(i_director.imdb_rating), -- 1ª Tentativa: Título + Diretor\n",
    "        AVG(i_year.imdb_rating)      -- 2ª Tentativa: Título + Ano\n",
    "    ) AS imdb_rating_concatenada\n",
    "    \n",
    "FROM netflix_directors_exploded AS n\n",
    "-- JOIN 1: Título + Diretor (Alta Precisão)\n",
    "LEFT JOIN imdb_full_directors AS i_director\n",
    "    ON n.title_clean = i_director.imdb_title_clean\n",
    "    AND n.director_clean = i_director.imdb_director_clean\n",
    "\n",
    "-- JOIN 2: Título + Ano (Média Precisão)\n",
    "LEFT JOIN imdb_full_year AS i_year\n",
    "    ON n.title_clean = i_year.imdb_title_clean\n",
    "    AND jaro_winkler_similarity(n.title_clean, i_year.imdb_title_clean) > 0.8\n",
    "\n",
    "GROUP BY\n",
    "    n.show_id, n.type, n.title, n.director, n.cast, n.country,\n",
    "    n.date_added, n.release_year, n.rating, n.duration,\n",
    "    n.listed_in, n.description, n.streaming, n.genres_processed\n",
    "\n",
    "HAVING\n",
    "    COALESCE(AVG(i_director.imdb_rating), AVG(i_year.imdb_rating)) IS NOT NULL\n",
    "\"\"\"\n",
    "\n",
    "print(\"Executando a consulta de 'merge' (isso pode levar alguns minutos)...\")\n",
    "# Executa a consulta e salva o resultado em um novo CSV\n",
    "con.execute(f\"\"\"\n",
    "    COPY ({merge_query})\n",
    "    TO '{output_csv_file}'\n",
    "    WITH (HEADER 1, DELIMITER ',')\n",
    "\"\"\")\n",
    "print(f\"\\nSucesso! Arquivo salvo como: '{output_csv_file}'\")\n",
    "\n",
    "# --- Verificação ---\n",
    "print(\"Verificando quantos títulos conseguimos concatenar...\")\n",
    "\n",
    "result = con.execute(f\"\"\"\n",
    "    SELECT\n",
    "        COUNT(*) AS total_titulos,\n",
    "        COUNT(imdb_rating_concatenada) AS titulos_com_nota\n",
    "    FROM read_csv_auto('{output_csv_file}', header=True)\n",
    "\"\"\").df()\n",
    "\n",
    "total = df_streaming.shape[0]\n",
    "matched = result['titulos_com_nota'].iloc[0]\n",
    "percent = (matched / total) * 100 if total > 0 else 0\n",
    "\n",
    "print(\"\\n--- Relatório de Concatenação ---\")\n",
    "print(f\"Total de títulos no seu CSV: {total}\")\n",
    "print(f\"Títulos que receberam nota do IMDb: {matched}\")\n",
    "print(f\"Taxa de sucesso da concatenação: {percent:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2673bdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_streaming.info()\n",
    "\n",
    "output_csv_file = os.path.join(imdb_data_dir, 'filmes_series_imdb.csv')\n",
    "df_streaming_completo = pd.read_csv(output_csv_file)\n",
    "\n",
    "#Dividir generos em varios booleanos\n",
    "df_streaming_completo['genres_processed'] = df_streaming_completo['genres_processed'].fillna('Unknown')\n",
    "genre_dummies = df_streaming_completo['genres_processed'].str.get_dummies(sep=', ')\n",
    "genre_dummies = genre_dummies.add_prefix('Genre_')\n",
    "df_streaming_completo = pd.concat([df_streaming_completo, genre_dummies], axis=1)\n",
    "df_streaming_completo.to_csv(output_csv_file)\n",
    "\n",
    "df_streaming_completo.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad9548c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remover_nulos_por_coluna(caminho_csv, nomes_colunas):\n",
    "    \"\"\"\n",
    "    Lê um arquivo CSV e retorna um DataFrame sem as linhas\n",
    "    onde a 'nomes_colunas' especificada está em branco (NaN ou string vazia).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 1. Lê o arquivo CSV\n",
    "        df = pd.read_csv(caminho_csv)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERRO: O arquivo '{caminho_csv}' não foi encontrado.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"ERRO ao ler o arquivo: {e}\")\n",
    "        return None\n",
    "\n",
    "    # 2. Verifica se a coluna existe no DataFrame\n",
    "    colunas_faltando = [col for col in nomes_colunas if col not in df.columns]\n",
    "    \n",
    "    if colunas_faltando:\n",
    "        print(f\"ERRO: As seguintes colunas não foram encontradas: {colunas_faltando}\")\n",
    "        print(f\"Colunas disponíveis: {df.columns.to_list()}\")\n",
    "        return None\n",
    "\n",
    "    # 3. Remove as linhas onde a coluna é Nula (NaN, pd.NA, etc.)\n",
    "    df_limpo = df.dropna(subset=nomes_colunas)\n",
    "\n",
    "    # 5. Devolve o DataFrame limpo\n",
    "    return df_limpo\n",
    "\n",
    "#Teste\n",
    "df = remover_nulos_por_coluna(output_csv_file, ['country'])\n",
    "\n",
    "df.info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
